{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tony/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import Lib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import datasets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import csv\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Importing the dataset\n",
    "dataset_train = pd.read_csv('train.csv', header=None)\n",
    "dataset_test = pd.read_csv('test.csv', header=None)\n",
    "\n",
    "# Take all columns except last one\n",
    "train = dataset_train.iloc[:, :-1]\n",
    "test = dataset_test.iloc[:, :]\n",
    "y = dataset_train.iloc[:, dataset_train.shape[1]-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "0            0\n",
      "1            0\n",
      "2            0\n",
      "3            0\n",
      "4            0\n",
      "5            0\n",
      "6            0\n",
      "7            0\n",
      "8            0\n",
      "9            0\n",
      "10           0\n",
      "11           0\n",
      "12           0\n",
      "13           0\n",
      "14           0\n",
      "15        3103\n",
      "16           0\n",
      "17           0\n",
      "18           0\n",
      "19           0\n",
      "20           0\n",
      "21           0\n",
      "22           0\n",
      "23           0\n",
      "24           0\n",
      "25           0\n",
      "26           0\n",
      "27           0\n",
      "28           0\n",
      "29           0\n",
      "         ...  \n",
      "13538        0\n",
      "13539        0\n",
      "13540        0\n",
      "13541        0\n",
      "13542        0\n",
      "13543        0\n",
      "13544        0\n",
      "13545        0\n",
      "13546        0\n",
      "13547        0\n",
      "13548        0\n",
      "13549        0\n",
      "13550        0\n",
      "13551        0\n",
      "13552        0\n",
      "13553        0\n",
      "13554        0\n",
      "13555        0\n",
      "13556        0\n",
      "13557        0\n",
      "13558        0\n",
      "13559        0\n",
      "13560        0\n",
      "13561        0\n",
      "13562    10520\n",
      "13563        0\n",
      "13564        0\n",
      "13565        0\n",
      "13566        0\n",
      "13567        0\n",
      "Name: 10, Length: 45222, dtype: int64\n",
      "Finish One Hot Enconding\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nn_com = 30\\npca = PCA(n_components=n_com)\\ndataset_pca = pca.fit_transform(dataset)\\nprint (\"Finish PCA preprocess\") \\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoding categorial data\n",
    "# Replace string by categories number\n",
    "drop = []\n",
    "train_objs_num = len(train)\n",
    "\n",
    "dataset = pd.concat(objs=[train, test], axis=0)\n",
    "\n",
    "# Salary Mapping\n",
    "edu_mapping = {\n",
    "        ' Preschool':0, \n",
    "        ' 1st-4th':1,\n",
    "        ' 5th-6th':2,\n",
    "        ' 7th-8th':3,\n",
    "        ' 9th':4,\n",
    "        ' 10th':5,\n",
    "        ' 11th':6,\n",
    "        ' 12th':7,\n",
    "        ' HS-grad':15,\n",
    "        ' Prof-school':70,\n",
    "        ' Assoc-acdm':25,\n",
    "        ' Assoc-voc':25,\n",
    "        ' Some-college':20,\n",
    "        ' Bachelors':40,\n",
    "        ' Masters':55,\n",
    "        ' Doctorate':70\n",
    "}\n",
    "gender_mapping = {\n",
    "    ' Female':10,\n",
    "    ' Male':30\n",
    "}\n",
    "color_mapping = {\n",
    "    ' Amer-Indian-Eskimo':11,\n",
    "    ' White':25, \n",
    "    ' Asian-Pac-Islander':25, \n",
    "    ' Other':10, \n",
    "    ' Black':13\n",
    "}\n",
    "marital_mapping={\n",
    "    ' Married-civ-spouse':45,\n",
    "    ' Divorced':10,\n",
    "    ' Never-married':5,\n",
    "    ' Separated':6,\n",
    "    ' Widowed':8,\n",
    "    ' Married-spouse-absent':8,\n",
    "    ' Married-AF-spouse':45\n",
    "}\n",
    "dataset[3] = dataset[3].map(edu_mapping)\n",
    "dataset[9] = dataset[9].map(gender_mapping)\n",
    "dataset[8] = dataset[8].map(color_mapping)\n",
    "dataset[5] = dataset[5].map(marital_mapping)\n",
    "\n",
    "income_minus = dataset.values[:,10] - dataset.values[:,11]\n",
    "print(income_minus)\n",
    "dataset.values[:,10] = income_minus\n",
    "print (dataset[10])\n",
    "\n",
    "dataset = dataset.drop(11, axis = 1)\n",
    "# One hot encoder\n",
    "'''\n",
    "for row in range(0, dataset.shape[1]):\n",
    "    if (isinstance(dataset.values[1][row], str)):        \n",
    "        print (\"Delete row: \", row), \n",
    "        one_hot = pd.get_dummies(dataset[row])\n",
    "        drop.append(row)\n",
    "        dataset = pd.concat([dataset, one_hot], axis=1)\n",
    "        dataset = dataset.iloc[:, :-1]\n",
    "    \n",
    "# Remove original attributes\n",
    "drop.sort(reverse=True)\n",
    "for row in drop:\n",
    "    dataset = dataset.drop(row, axis = 1)\n",
    "'''\n",
    "\n",
    "# Only label encoder\n",
    "for row in range(0, dataset.shape[1]):\n",
    "    if (isinstance(dataset.values[1][row], str)):    \n",
    "        labelencoder = LabelEncoder()\n",
    "        target = labelencoder.fit_transform(dataset.values[:, row])\n",
    "        dataset[row] = target\n",
    "\n",
    "print (\"Finish One Hot Enconding\")\n",
    "\n",
    "# PCA\n",
    "'''\n",
    "n_com = 30\n",
    "pca = PCA(n_components=n_com)\n",
    "dataset_pca = pca.fit_transform(dataset)\n",
    "print (\"Finish PCA preprocess\") \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified sample 487\n",
      "Train Accuracy:  0.8501825329963494\n",
      "Test Accuracy:  0.8461781427668983\n",
      "precision:  0.8461781427668983\n",
      "recall:  0.8461781427668983\n",
      "fscore:  0.8461781427668983\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "X = dataset[:train_objs_num]\n",
    "test_data = dataset[train_objs_num:]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 0)\n",
    "\n",
    "#feature normalize\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "\n",
    "svm = SVC(kernel='rbf')\n",
    "svm.fit(X_train_std, y_train)\n",
    "y_pred = svm.predict(X_test_std)\n",
    "\n",
    "print (\"Misclassified sample %d\" % (y_test!=y_pred).sum())\n",
    "print (\"Train Accuracy: \", accuracy_score(y_train, svm.predict(X_train_std)))\n",
    "print (\"Test Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test, y_pred, average='micro')\n",
    "print (\"precision: \", precision)\n",
    "print (\"recall: \", recall)\n",
    "print (\"fscore: \", fscore)\n",
    "\n",
    "\n",
    "test_std = sc.transform(test_data)\n",
    "test_std_pred = svm.predict(test_std)\n",
    "\n",
    "with open('output.csv', 'w+') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['ID', 'ans'])\n",
    "    for i in range(len(test_std_pred)):\n",
    "        writer.writerow([i, test_std_pred[i]])\n",
    "\n",
    "print (\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: ' United-States'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-71d317d21ee0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m '''    \n\u001b[1;32m     17\u001b[0m \u001b[0mforest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'entropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mforest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Misclassified sample %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \"\"\"\n\u001b[1;32m    246\u001b[0m         \u001b[0;31m# Validate or convert input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    431\u001b[0m                                       force_all_finite)\n\u001b[1;32m    432\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: ' United-States'"
     ]
    }
   ],
   "source": [
    "X = dataset[:train_objs_num]\n",
    "test_data = dataset[train_objs_num:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "'''\n",
    "max_fscore = 0\n",
    "target_n_estimators = 0\n",
    "for i in range(10, 100):\n",
    "    forest = RandomForestClassifier(criterion='entropy', n_estimators=i)\n",
    "    forest.fit(X_train, y_train)\n",
    "    y_pred = forest.predict(X_test)\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(y_test, y_pred, average='micro')\n",
    "    if fscore > max_fscore:\n",
    "        max_fscore = fscore\n",
    "        target_n_estimators = i\n",
    "'''    \n",
    "forest = RandomForestClassifier(criterion='entropy', n_estimators=20)\n",
    "forest.fit(X_train, y_train)\n",
    "y_pred = forest.predict(X_test)\n",
    "print (\"Misclassified sample %d\" % (y_test!=y_pred).sum())\n",
    "print (\"Train Accuracy: \", accuracy_score(y_train, forest.predict(X_train)))\n",
    "print (\"Test Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test, y_pred, average='micro')\n",
    "print (\"precision: \", precision)\n",
    "print (\"recall: \", recall)\n",
    "print (\"fscore: \", fscore)\n",
    "\n",
    "\n",
    "test_std_pred = forest.predict(test_data)\n",
    "\n",
    "with open('output.csv', 'w+') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['ID', 'ans'])\n",
    "    for i in range(len(test_std_pred)):\n",
    "        writer.writerow([i, test_std_pred[i]])\n",
    "\n",
    "print (\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, numpy.int64 found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-af001cdd1164>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mxgbc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mxgbc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgbc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Misclassified sample %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set)\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m             train_dmatrix = DMatrix(X, label=training_labels,\n\u001b[0;32m--> 541\u001b[0;31m                                     missing=self.missing, nthread=self.n_jobs)\n\u001b[0m\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m         self._Booster = train(xgb_options, train_dmatrix, self.n_estimators,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, label, missing, weight, silent, feature_names, feature_types, nthread)\u001b[0m\n\u001b[1;32m    342\u001b[0m         data, feature_names, feature_types = _maybe_pandas_data(data,\n\u001b[1;32m    343\u001b[0m                                                                 \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m                                                                 feature_types)\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         data, feature_names, feature_types = _maybe_dt_data(data,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_maybe_pandas_data\u001b[0;34m(data, feature_names, feature_types)\u001b[0m\n\u001b[1;32m    212\u001b[0m         msg = \"\"\"DataFrame.dtypes for data must be int, float or bool.\n\u001b[1;32m    213\u001b[0m                 Did not expect the data types in fields \"\"\"\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbad_fields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfeature_names\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, numpy.int64 found"
     ]
    }
   ],
   "source": [
    "X = dataset[:train_objs_num]\n",
    "test_data = dataset[train_objs_num:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 0)\n",
    "\n",
    "\n",
    "xgbc = XGBClassifier(max_depth=10)\n",
    "xgbc.fit(X_train, y_train)\n",
    "y_pred = xgbc.predict(X_test)\n",
    "print (\"Misclassified sample %d\" % (y_test!=y_pred).sum())\n",
    "print (\"Train Accuracy: \", accuracy_score(y_train, xgbc.predict(X_train)))\n",
    "print (\"Test Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test, y_pred, average='micro')\n",
    "print (\"precision: \", precision)\n",
    "print (\"recall: \", recall)\n",
    "print (\"fscore: \", fscore)\n",
    "\n",
    "\n",
    "test_std_pred = xgbc.predict(test_data)\n",
    "\n",
    "with open('output.csv', 'w+') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['ID', 'ans'])\n",
    "    for i in range(len(test_std_pred)):\n",
    "        writer.writerow([i, test_std_pred[i]])\n",
    "\n",
    "print (\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_attribute = 1\n",
    "dic = dict()\n",
    "for i in range(0, dataset_train.shape[0]):\n",
    "    if (dataset_train.values[i][14] == 1):\n",
    "        string = dataset_train.values[i][target_attribute]\n",
    "        if string in dic:\n",
    "            dic[string] += 1\n",
    "        else:\n",
    "            dic[string] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
