{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Lib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import datasets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import csv\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Importing the dataset\n",
    "dataset_train = pd.read_csv('train.csv', header=None)\n",
    "dataset_test = pd.read_csv('test.csv', header=None)\n",
    "\n",
    "# Take all columns except last one\n",
    "train = dataset_train.iloc[:, :-1]\n",
    "test = dataset_test.iloc[:, :]\n",
    "y = dataset_train.iloc[:, dataset_train.shape[1]-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish One Hot Enconding\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nn_com = 30\\npca = PCA(n_components=n_com)\\ndataset_pca = pca.fit_transform(dataset)\\nprint (\"Finish PCA preprocess\") \\n'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoding categorial data\n",
    "# Replace string by categories number\n",
    "drop = []\n",
    "train_objs_num = len(train)\n",
    "\n",
    "dataset = pd.concat(objs=[train, test], axis=0)\n",
    "\n",
    "# Salary Mapping\n",
    "edu_mapping = {\n",
    "        ' Preschool':0, \n",
    "        ' 1st-4th':1,\n",
    "        ' 5th-6th':2,\n",
    "        ' 7th-8th':3,\n",
    "        ' 9th':4,\n",
    "        ' 10th':5,\n",
    "        ' 11th':6,\n",
    "        ' 12th':7,\n",
    "        ' HS-grad':15,\n",
    "        ' Prof-school':70,\n",
    "        ' Assoc-acdm':25,\n",
    "        ' Assoc-voc':25,\n",
    "        ' Some-college':20,\n",
    "        ' Bachelors':40,\n",
    "        ' Masters':55,\n",
    "        ' Doctorate':70\n",
    "}\n",
    "gender_mapping = {\n",
    "    ' Female':10,\n",
    "    ' Male':30\n",
    "}\n",
    "color_mapping = {\n",
    "    ' Amer-Indian-Eskimo':11,\n",
    "    ' White':25, \n",
    "    ' Asian-Pac-Islander':25, \n",
    "    ' Other':10, \n",
    "    ' Black':13\n",
    "}\n",
    "marital_mapping={\n",
    "    ' Married-civ-spouse':45,\n",
    "    ' Divorced':10,\n",
    "    ' Never-married':5,\n",
    "    ' Separated':6,\n",
    "    ' Widowed':8,\n",
    "    ' Married-spouse-absent':8,\n",
    "    ' Married-AF-spouse':45\n",
    "}\n",
    "dataset[3] = dataset[3].map(edu_mapping)\n",
    "dataset[9] = dataset[9].map(gender_mapping)\n",
    "dataset[8] = dataset[8].map(color_mapping)\n",
    "dataset[5] = dataset[5].map(marital_mapping)\n",
    "\n",
    "# One hot encoder\n",
    "'''\n",
    "for row in range(0, dataset.shape[1]):\n",
    "    if (isinstance(dataset.values[1][row], str)):        \n",
    "        print (\"Delete row: \", row), \n",
    "        one_hot = pd.get_dummies(dataset[row])\n",
    "        drop.append(row)\n",
    "        dataset = pd.concat([dataset, one_hot], axis=1)\n",
    "        dataset = dataset.iloc[:, :-1]\n",
    "    \n",
    "# Remove original attributes\n",
    "drop.sort(reverse=True)\n",
    "for row in drop:\n",
    "    dataset = dataset.drop(row, axis = 1)\n",
    "'''\n",
    "\n",
    "# Only label encoder\n",
    "for row in range(0, dataset.shape[1]):\n",
    "    if (isinstance(dataset.values[1][row], str)):    \n",
    "        labelencoder = LabelEncoder()\n",
    "        target = labelencoder.fit_transform(dataset.values[:, row])\n",
    "        dataset[row] = target\n",
    "\n",
    "print (\"Finish One Hot Enconding\")\n",
    "\n",
    "# PCA\n",
    "'''\n",
    "n_com = 30\n",
    "pca = PCA(n_components=n_com)\n",
    "dataset_pca = pca.fit_transform(dataset)\n",
    "print (\"Finish PCA preprocess\") \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified sample 487\n",
      "Train Accuracy:  0.8501825329963494\n",
      "Test Accuracy:  0.8461781427668983\n",
      "precision:  0.8461781427668983\n",
      "recall:  0.8461781427668983\n",
      "fscore:  0.8461781427668983\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "X = dataset[:train_objs_num]\n",
    "test_data = dataset[train_objs_num:]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 0)\n",
    "\n",
    "#feature normalize\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "\n",
    "svm = SVC(kernel='rbf')\n",
    "svm.fit(X_train_std, y_train)\n",
    "y_pred = svm.predict(X_test_std)\n",
    "\n",
    "print (\"Misclassified sample %d\" % (y_test!=y_pred).sum())\n",
    "print (\"Train Accuracy: \", accuracy_score(y_train, svm.predict(X_train_std)))\n",
    "print (\"Test Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test, y_pred, average='micro')\n",
    "print (\"precision: \", precision)\n",
    "print (\"recall: \", recall)\n",
    "print (\"fscore: \", fscore)\n",
    "\n",
    "\n",
    "test_std = sc.transform(test_data)\n",
    "test_std_pred = svm.predict(test_std)\n",
    "\n",
    "with open('output.csv', 'w+') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['ID', 'ans'])\n",
    "    for i in range(len(test_std_pred)):\n",
    "        writer.writerow([i, test_std_pred[i]])\n",
    "\n",
    "print (\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified sample 934\n",
      "Train Accuracy:  0.9956956126841211\n",
      "Test Accuracy:  0.8524719633549203\n",
      "precision:  0.8524719633549203\n",
      "recall:  0.8524719633549203\n",
      "fscore:  0.8524719633549203\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "X = dataset[:train_objs_num]\n",
    "test_data = dataset[train_objs_num:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "'''\n",
    "max_fscore = 0\n",
    "target_n_estimators = 0\n",
    "for i in range(10, 100):\n",
    "    forest = RandomForestClassifier(criterion='entropy', n_estimators=i)\n",
    "    forest.fit(X_train, y_train)\n",
    "    y_pred = forest.predict(X_test)\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(y_test, y_pred, average='micro')\n",
    "    if fscore > max_fscore:\n",
    "        max_fscore = fscore\n",
    "        target_n_estimators = i\n",
    "'''    \n",
    "forest = RandomForestClassifier(criterion='entropy', n_estimators=20)\n",
    "forest.fit(X_train, y_train)\n",
    "y_pred = forest.predict(X_test)\n",
    "print (\"Misclassified sample %d\" % (y_test!=y_pred).sum())\n",
    "print (\"Train Accuracy: \", accuracy_score(y_train, forest.predict(X_train)))\n",
    "print (\"Test Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test, y_pred, average='micro')\n",
    "print (\"precision: \", precision)\n",
    "print (\"recall: \", recall)\n",
    "print (\"fscore: \", fscore)\n",
    "\n",
    "\n",
    "test_std_pred = forest.predict(test_data)\n",
    "\n",
    "with open('output.csv', 'w+') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['ID', 'ans'])\n",
    "    for i in range(len(test_std_pred)):\n",
    "        writer.writerow([i, test_std_pred[i]])\n",
    "\n",
    "print (\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tony/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified sample 425\n",
      "Train Accuracy:  0.903784049424319\n",
      "Test Accuracy:  0.8657612128869235\n",
      "precision:  0.8657612128869235\n",
      "recall:  0.8657612128869235\n",
      "fscore:  0.8657612128869235\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tony/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/home/tony/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "X = dataset[:train_objs_num]\n",
    "test_data = dataset[train_objs_num:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 0)\n",
    "\n",
    "\n",
    "xgbc = XGBClassifier(max_depth=10)\n",
    "xgbc.fit(X_train, y_train)\n",
    "y_pred = xgbc.predict(X_test)\n",
    "print (\"Misclassified sample %d\" % (y_test!=y_pred).sum())\n",
    "print (\"Train Accuracy: \", accuracy_score(y_train, xgbc.predict(X_train)))\n",
    "print (\"Test Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test, y_pred, average='micro')\n",
    "print (\"precision: \", precision)\n",
    "print (\"recall: \", recall)\n",
    "print (\"fscore: \", fscore)\n",
    "\n",
    "\n",
    "test_std_pred = xgbc.predict(test_data)\n",
    "\n",
    "with open('output.csv', 'w+') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['ID', 'ans'])\n",
    "    for i in range(len(test_std_pred)):\n",
    "        writer.writerow([i, test_std_pred[i]])\n",
    "\n",
    "print (\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
